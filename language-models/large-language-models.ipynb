{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install openai\n",
    "%pip install tiktoken\n",
    "%pip install unstructured\n",
    "%pip install chromadb\n",
    "%pip install pdfminer.six\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# You would need to \n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_KEY')\n",
    "os.chdir('/Users/tcoan/git_repos/ncrm-spring-school/')\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI's API (GPT-3)\n",
    "\n",
    "Sending requests to the OpenAI API is quite easy using LangChain. Start by importing the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then instantiate the `OpenAI()` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the \"temperature\" to zero to remove randomness in the response\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to send a (correctly formatted) prompt to the API and collect the response. Note that the `langchain` library offers a number of different \"templates\" to help structure your prompts. Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_template = \"\"\"\n",
    "Here is an example of a movie review:\n",
    "\n",
    "{review}\n",
    "\n",
    "Is this a positive, negative, or neutral review?\" If you don't know, say 'unclear'. \\\n",
    "Return the result as {response_format} with the key 'sentiment'.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"review\", \"response_format\"],\n",
    "    template=sentiment_template,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a \"formatted\" prompt would look like with input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review = 'I know that most people love the movie Titanic. I thought it was pretty stupid. Sappy!'\n",
    "response_format = 'JSON'\n",
    "print(prompt.format(review=movie_review, response_format=response_format))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just pass the formatted prompt to our `llm` object and collect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = llm(prompt.format(review=movie_review, response_format=response_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_json)\n",
    "print(type(response_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = print(json.loads(response_json))\n",
    "print(type(response_json))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the coolest features of using a GPT-style model for doing classification is that we can ask the model to justify its decision (in natural language!). Let's change our prompt to ask for a justification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_template = \"\"\"\n",
    "Here is an example of a movie review:\n",
    "\n",
    "{review}\n",
    "\n",
    "Is this a positive, negative, or neutral review?\" If you don't know, say 'unclear'. \\\n",
    "Return the result as JSON with the key 'sentiment' and return a short \\\n",
    "description of why you gave this answer using the key \"reason\". \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template=sentiment_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json = llm(prompt.format(review=movie_review))\n",
    "print(response_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interacting with the OpenAI's API will often provide the most accurate models (especially `davinci`), but it is expensive to use. Let's look at the \"Chat\" version (GPT3.5 turbo) which is 10 times cheaper!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT implementation (GPT3.5 turbo)\n",
    "\n",
    "To interact with the \"Chat\" versions of OpenAI models using `langchain`, we need to load some additional classes/functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then instantiate the `ChatOpenAI()` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat mode instance\n",
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you pass prompts to `ChatOpenAI` differs from the standard API as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review = 'I know that most people love the movie Titanic. I thought it was pretty stupid. Sappy!'\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that can classify the sentiment of movie review texts. The labels you can use are positive, negative and neutral.\"),\n",
    "    HumanMessage(content=f\"Provide label for the following review: {movie_review}\\n\\nMake sure that your label is one of the following: 'positive', 'negative' or 'neutral'. Also provide a short justification for your label. Return the response as JSON.\"),\n",
    "]\n",
    "\n",
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)\n",
    "#print(json.loads(response.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot classification of movie reviews data\n",
    "\n",
    "It's easy to do \"zero-shot\" classification with GPT-style models, but the obvious question is how to the perform? To get a sense of performance, let's load our movie reviews data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/movie_reviews.csv').to_dict('records')\n",
    "\n",
    "# Subset the last 500 reviews which we used as a \"test set\"\n",
    "reviews_unlabeled = reviews[1500:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply need to loop over our text, send it to OpenAI, and collect the predictions. To make this easier, let's define a function to send each request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_request(movie_review):\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that can classify the sentiment of movie review texts. The labels you can use are positive, negative and neutral.\"),\n",
    "        HumanMessage(content=f\"Provide label for the following review: {movie_review}\\n\\nMake sure that your label is one of the following: 'positive', 'negative' or 'neutral'. Also provide a short justification for your label. You MUST return the response as JSON.\"),\n",
    "    ]\n",
    "    return chat(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've already ran this loop and saved the JSON responses here:\n",
    "preds = pd.read_json('language-models/zero_shot_predictions.json').to_dict('records')\n",
    "\n",
    "# I don't want to run this again, so I'm commenting this out!\n",
    "\"\"\"\"\n",
    "results_chat = []\n",
    "for i,row in enumerate(reviews_unlabeled):\n",
    "    response = send_chat_request(row['text'])\n",
    "    results_chat.append(response.content)\n",
    "    print(f'Finished iteration {i}')\n",
    "\"\"\"\n",
    "\n",
    "print(preds[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the \"positive\" predictions and save as a numpy array\n",
    "y_pred = np.array([row['positive'] for row in preds])\n",
    "\n",
    "# Get the \"truth\" and save as a numpy array:\n",
    "y = np.array([row['positive'] for row in reviews_unlabeled])\n",
    "\n",
    "print(accuracy_score(y, y_pred))\n",
    "print(precision_score(y, y_pred))\n",
    "print(recall_score(y, y_pred))\n",
    "print(f1_score(y, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot learning with LLMs and LangChain\n",
    "\n",
    "While we provided no \"training\" data to the model in the zero-shot example, it is often helpful to nudge the model in the right direction. You can do this via \"few shot\" learning: i.e., you provide a \"few\" examples from the data to the prompt prior to classification. `langchain` makes doing few shot learning with OpenAI (and may other models!) relatively painless. Start by importing the few shot learning prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's set up a few examples of movie reviews with different labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {'text': 'The movie is about two teen couples who drink and drive, resulting in an accident where one of the guys dies. His girlfriend continues to see him in her life and has nightmares. The film attempts to present a cool idea but fails to execute it properly, resulting in a confusing and jumbled plot. The actors are good, but the film lacks entertainment value and feels redundant. It is not a horror or teen slasher flick, but it is packaged to look that way. The reviewer suggests skipping it.',\n",
    "     'label': 'negative'},\n",
    "    {'text':'\"From Hell\" is a successful film adaptation of a graphic novel by Alan Moore and Eddie Campbell about the Jack the Ripper murders in 1888 London\\'s East End. The film is directed by the Hughes brothers and stars Johnny Depp as Inspector Frederick Abberline, who investigates the gruesome murders with the help of an unfortunate named Mary Kelly (Heather Graham). The film has a unique and interesting theory about the identity of the killer and the reasons he chooses to slay. The film\\'s appearance is dark and bleak, capturing the dreariness of Victorian-era London, and the acting is solid, with Depp and Graham turning in strong performances. The film is rated R for strong violence/gore, sexuality, language, and drug content.',\n",
    "     'label':'positive'},\n",
    "    {'text':'The \"The Bourne Identity\" is an okay movie. It has all of the typical action that you would expect and the acting is decent. It is entertaining enough to watch.',\n",
    "     'label':'neutral'}\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to set up our prompt. This typically includes defining a prefix for out prompt, and example (or data) template, and a prompt suffix. Let's do this for our movie review examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"You are a helpful assistant that can classify the sentiment of movie review texts. \n",
    "The labels you can use are positive, negative and neutral. Here are some examples\"\"\"\n",
    "\n",
    "example_template = \"\"\"\n",
    "Text: {text}\n",
    "Label: {label}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"label\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "suffix = \"\"\"\n",
    "Text: {text}\n",
    "Label: \"\"\"\n",
    "\n",
    "# Now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"text\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then view the formatted prompt in the usual way by calling the `.format()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review = \"The Big Lebowski is the funniest movie that I've ever seen. I know this sort of comedy isn't for everyone, but wow.\"\n",
    "print(few_shot_prompt_template.format(text=movie_review))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we send the prompt to OpenAI in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the \"temperature\" to zero to remove randomness in the response\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "response = llm(few_shot_prompt_template.format(text=movie_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization with LangChain\n",
    "\n",
    "Note that when entering \"examples\" for the few shot learning prompt above, I didn't enter full movie reviews but instead only summaries. How did I get these summaries? I used LangChain! To do text summarization, we start by loading the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And initialize the model that we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that that the number of tokens that you can pass to OpenAI is limited, so when summarizing long documents, it's necessary to split them into smaller chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter()\n",
    "texts = text_splitter.split_text(reviews[1]['text'])\n",
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(chat, chain_type=\"refine\")\n",
    "chain.run(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering\n",
    "\n",
    "One of the most powerful features of LangChain + GPT-style models is that they provide the ability to do question answering over a set of documents using natural language. As an example, let's load the `trump_tweets_2017.csv` and use `langchain` to make queries on the tweet content. First, load the CSV \"agent\" needed to interact with CSVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_csv_agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model and load the data into the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_csv_agent(OpenAI(temperature=0), 'data/trump_tweets_2017.csv', verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run queries against the Trump data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"Can you provide 5 tweets that attacks Democracts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"Show me the tweets that related the environment. Don't include tweets on the business environment.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question answering with a PDF file\n",
    "\n",
    "We are not limited to making queries againt as CSV -- `langchain` provides functionality to query almost anything! For example, let's see how to load and run queries on a PDF document. Start by loading the necessary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "loader = UnstructuredPDFLoader(\"/Users/tcoan/Downloads/s41598-021-01714-4.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `VectorstoreIndexCreator()` load and prepare our data. This is where all of the magic happens. `VectorstoreIndexCreator()` is carries at the following tasks:\n",
    "\n",
    "1. Loading the PDF and splitting it into smaller chunks.\n",
    "2. Creating embeddings for each document (i.e., chunk)\n",
    "3. Storing the documents and embeddings in a vectorstore (i.e., a database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the `index` object (and the underlying vectorstore) to make queries like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.query('does the paper use RoBERTa for classification?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.query('what is the F1 score for the best performing model in the paper?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Al Gore is an alarmist and we should not take anything he says seriously.'\n",
    "prompt = f'Is the follow sentence an example of climate skepticism: {text}. If so, why?'\n",
    "index.query(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0db7a017d97a45ffea759374b98e80b051fa13973580c065a6b8f6e28c7ab80d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
