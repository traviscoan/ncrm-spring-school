{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning: the standard pipeline\n",
    "\n",
    "This notebook introduces **supervised learning** in Python. As described more fully in lecture, \"supervised\" learning represents a set of algorithms that classify items (e.g., a document, paragraph, or sentence) into predefined labels (or classes) based on human-annotated \"training\" data.\n",
    "\n",
    "We will outline a general **pipeline** that you can follow when working with supervised text classification. The pipeline includes:\n",
    "\n",
    "1. Collecting training data\n",
    "2. Text preprocessing and normalization\n",
    "3. Feature extraction\n",
    "4. Model estimation and selection (and parameter tuning)\n",
    "5. Final model estimation and reporting performance\n",
    "\n",
    "This notebook covers items 2-5 in Python and `sklearn`. Before getting started, let's load the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/tcoan/git_repos/ncrm-spring-school')\n",
    "\n",
    "import pandas as pd # Use to read data\n",
    "import numpy as np  # Numpy is used in various places\n",
    "\n",
    "# Tokenization, stopword removal, and lemmatization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load functions for model selection and performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data\n",
    "\n",
    "There are a number of different ways to collect training data. In the social sciences, we tend to use the standard operating procedure of quantitative content analysis:\n",
    "\n",
    "1. Construct **coding guidelines** to code into documents into substantively meaningful classes based on relevant theory.\n",
    "2. Train coders on a small, random sample of documents, **calculate reliability**, and discuss disagreements. **Repeat** this process until reliability is sufficiently high}.\n",
    "3. Code the remaining documents in the training set. Ideally, you would have multiple coders per document; however, it is common to have a single coder after the training phase.\n",
    "\n",
    "See <a href=\"https://www.amazon.co.uk/Content-Analysis-Introduction-Its-Methodology/dp/1412983150\">Krippendorff's (2013) classic book</a> for a detailed introduction. More recently, researchers have been using crowd-sourcing to annotate documents. For a detailed introduction to this approach, see <a href=\"http://kenbenoit.net/pdfs/Crowd_sourced_data_coding_APSR.pdf\">Benoit et al. (2016)</a>.\n",
    "\n",
    "### Movie reviews\n",
    "\n",
    "We will use the well-known \"move review\" data from [Pang and Lee's (2004)](http://www.cs.cornell.edu/people/pabo/movie-review-data/) article on using supervised methods for sentiment analysis. This is the same data that we used for lexicon-based sentiment analysis several weeks ago. To refresh, the data has the following fields:\n",
    "\n",
    "* **id**: A unique ID for each movie review.\n",
    "* **class**: An integer equal to 1 if the review is positive and 0 if negative.\n",
    "* **text**: The text content of each review (already converted to lowercase).\n",
    "\n",
    "We can load the data in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_csv('data/movie_reviews.csv')\n",
    "\n",
    "# Convert to a list of dicts!\n",
    "reviews = reviews_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can examine a single \"review\" as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the last review\n",
    "print(reviews[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing and normalization\n",
    "\n",
    "As with dictionary and unsupervised approaches, the first step is **preprocessing**. We will do the following preprocessing:\n",
    "\n",
    "1. **Tokenize** into unigrams (i.e., words)\n",
    "2. **Remove stopwords and punctuation**\n",
    "\n",
    "Let's start with **tokenization**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize entire corpus of reviews\n",
    "tokens = [word_tokenize(row['text']) for row in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remove common **stopwords** and **punctuation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load common English stopwords\n",
    "stops = stopwords.words('english')\n",
    "extended_stops = stops + ['``', \"'s\", \"'ve'\"]\n",
    "\n",
    "# Define a function that removes stopwords AND punctuation\n",
    "def remove_stops(text, stops):\n",
    "    return [token for token in text if (token.lower() not in set(stops)) and (len(token) > 1)]\n",
    "\n",
    "tokens = [remove_stops(doc, extended_stops) for doc in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for sklearn. First, get the text\n",
    "texts = [' '.join(doc) for doc in tokens]\n",
    "\n",
    "# And then get the \"class\" variable\n",
    "positive = [trav['positive'] for trav in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While all of our 2,000 movie reviews are labeled, in real-world analysis, you will have (labeled) training data, as well as unlabeled data. We will simulate this more realistic scenario by \"pretending\" that the last 500 reviews are unlabeled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split texts\n",
    "texts_labeled = texts[0:1500]\n",
    "texts_unlabeled = texts[1500:]\n",
    "\n",
    "# Split class\n",
    "positive_labeled = positive[0:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labeled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "**Feature extraction** (also called **vectorization**) is a process whereby we extract meaningful features or attributes from raw textual data that will be used in a classification algorithm. We have already extracted **bag-of-words** features using the `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the text data using the chosen vectorizer\n",
    "X = vectorizer.fit_transform(texts_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `CountVectorizer()` actually do? It generates what's called a **document-term matrix** in which our documents are on the rows, our **vocabulary** (all of the unique terms in our corpus) is in the columns, and the cells represent the number of times (the count) a term shows up in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 1500 documents (which we already knew) and 35,162 unique words (or tokens) in our corpus. We can look at these 35k tokens using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can lookup the number of times the token '0009f' is used in the first movie review as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't show up! This isn't really surprising: only a handful of the 35k works will show up in any one movie review. Most of the cells in the **document-term matrix** will be zeros. This is what is called a **sparse matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Another commonly used vectorization method is applying **term frequency-inverse document frequency** weights.\n",
    "\n",
    "The TF-IDF weights are defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "tf * idf\n",
    "\\end{equation}\n",
    "\n",
    "Where $tf$ represents the relative term frequency of word in a document and the $idf$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "idf(t) = 1 + log\\frac{N}{1+df(t)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $N$ represents the total number of documents in the corpus and $df(t)$ represents the number of documents in which term $t$ is present. Often the $tfidf$ weigthts are normalized to range between 0 and 1 by using the L2 norm:\n",
    "\n",
    "\\begin{equation}\n",
    "tfidf_{norm} = \\frac{tfidf}{||tfidf||}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF vectorization in `sklearn` is easy. We simply load the the `TfidfVectorizer` and perform the same process as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate TF-IDF weights\n",
    "X_tfidf = vectorizer.fit_transform(texts_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our counts and TF-IDF weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Counts =')\n",
    "print(X[0,np.nonzero(X_tfidf[0,:])[1]].todense())\n",
    "\n",
    "print('TF-IDF weights =')\n",
    "print(X_tfidf[0,np.nonzero(X_tfidf[0,:])[1]].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TfidfVectorizer` smooths the IDF (i.e., adds a 1 to avoid division by 0) and normalizes using the **L2 norm** by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other feature extraction decisions\n",
    "\n",
    "There are a number of other items to consider when vectorizing. One thing that can really change classification performance is switching from **unigrams** to **ngrams**. This is easy to implement in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use unigrams and bigrams\n",
    "vectorizer_bigram = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Generate TF-IDF weights\n",
    "X_bigram = vectorizer_bigram.fit_transform(texts_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also sometimes restrict the maximum and minimum documents for which a token appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use unigrams and bigrams\n",
    "vectorizer_bigram = TfidfVectorizer(max_df = .95, min_df = 10, ngram_range=(1,2))\n",
    "\n",
    "# Generate TF-IDF weights\n",
    "X_bigram = vectorizer_bigram.fit_transform(texts_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a naive Bayes model in `sklearn`\n",
    "\n",
    "Fitting our model in `sklearn` is really, really easy. Let's get a feel for how this works using a simple **naive Bayes** model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the text data using the chosen vectorizer\n",
    "X = vectorizer.fit_transform(texts_labeled)\n",
    "\n",
    "# Prepare our \"class\" variable\n",
    "y = np.array(positive_labeled)\n",
    "\n",
    "# Initialize the classifer\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the model\n",
    "clf_fit = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate predictions for reviews (e.g., our `y` variable) using the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf_fit.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_predict[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can quickly look at how accurate our model is at predicting the movie reviews based on our text (`X`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fit.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wowzers, 98% accurate! We are done, right? foo <font color='red'>WRONG</font>: we really care about **in-sample** fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance\n",
    "\n",
    "How well does our model perform? Let's get a sense of how one would assess **out-of-sample** performance using `sklearn`. First, we need to split our data into a training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1234)\n",
    "\n",
    "print(\"Training set size = %s\" % y_train.shape)\n",
    "print(\"Validation set size = %s\" % y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model using the training set and predict the classes for our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifer\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the model\n",
    "clf_fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_predict = clf_fit.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "\n",
    "With our prediction in hand, we can now calculate a range of **performance metrics**. All of these metrics start with what's referred to as a **confusion matrix**: \n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:712/format:webp/1*Z54JgbS4DUwWSknhDCvNTQ.png\">\n",
    "\n",
    "From this matrix, we can define the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    Acurracy = \\frac{TP + TN}{TP + FP + FN + TN}\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    Precision = \\frac{TP}{TP + FP}\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    Recall = \\frac{TP}{TP + FN}\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    Specificity = \\frac{TN}{TN + FP}\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly employed metric is **accuracy**. However, we need to be very careful when using accuracy alone. For example, consider Dallas Raines, the most accurate meteorologist in history:\n",
    "\n",
    "<img src=\"http://farm6.static.flickr.com/5260/5516412091_06fea7fdb8.jpg\" style=\"width=400;height=300\">\n",
    "\n",
    "If accuracy falls apart for \"imbalanced classes,\" then what are our alternatives. The most common alternatives (and what I tend to use in my own work) is some combination of **precision** and **recall**:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" style=\"width=300;height=400\">\n",
    "\n",
    "These measures are formally combined in the **F1-score**:\n",
    "\n",
    "\\begin{equation}\n",
    "  F1 = \\frac{2\\:x\\:Precision\\:x\\:Recall}{Precision\\:+\\:Recall}\n",
    "\\end{equation}\n",
    "\n",
    "The F1-score takes into account the tradeoff between precision and accuracy. All of these metrics (and others as well) are easy to calculate in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy = {clf_fit.score(X_test, y_test)}')\n",
    "print(f'Precision = {precision_score(y_test, y_predict)}')\n",
    "print(f'Recall = {recall_score(y_test, y_predict)}')\n",
    "print(f'F1 score = {f1_score(y_test, y_predict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "In the last section, we randomly split our data into 70% training and 30% testing. The realized sample, however, is just one of many samples that we could have pulled. Here, we will look at **k-fold cross-validation**. What do we mean be cross-validation? Take a look at the following:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg\">\n",
    "\n",
    "Once again, cross-validation is easy in `sklearn`. Here is the relevant code for 10-fold CV using our movies data the **long way**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Get the k folds\n",
    "kf = KFold(n_splits=10, shuffle = True, random_state=50)\n",
    "\n",
    "# Loop over folds and calculate performance measure\n",
    "results = []\n",
    "for k, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    # Fit model\n",
    "    cfit = clf.fit(X[train_idx], y[train_idx])\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = cfit.predict(X[test_idx])\n",
    "    \n",
    "    # Write results\n",
    "    result = {'fold': k,\n",
    "              'precision': precision_score(y[test_idx], y_pred),\n",
    "              'recall': recall_score(y[test_idx], y_pred),\n",
    "              'f1': f1_score(y[test_idx], y_pred)}\n",
    "              \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then pull out any information from our `results` dictionary that we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average F1 score\n",
    "mean_f1 = np.mean(np.array([row['f1'] for row in results]))\n",
    "\n",
    "print(\"Average, cross-validated F1 score = %s\" % mean_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this code clearly outlines the steps taken for cross-validation, it is a bit cumbersome. Luckily, `sklearn` has a utilty function to streamline this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_val_score(clf, X, y, cv=10, scoring=\"precision\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can average over the each fold using `np.mean()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(clf, X, y, cv=10, scoring=\"precision\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "We've seen how to estimate a Naive Bayes classifier. However, this is only one of many models available in `sklearn` (for a list of available classifiers, see http://scikit-learn.org/stable/supervised_learning.html). When selecting a model, the typical process is to try a bunch of different models and select the \"best\" based on out of sample performance.\n",
    "\n",
    "Let's take a look at one model that has been shown to be extremely accurate for text classification problems: the **support vector machine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines\n",
    "\n",
    "There are two versions of **support vector classifiers** available in `sklearn`, the `LinearSVC()` (which is restricted to the linear case, but is super fast!) and the `SVC()` implementation (which is highly flexible, but a bit slow). For an extremely clear introduction to what these models are actually doing, see <a href=\"https://www.youtube.com/watch?v=N1vOgolbjSc\">this video</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit these models using the exact same procedure as specified above for Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the TF-IDF weights instead of counts\n",
    "X = X_tfidf\n",
    "\n",
    "# Split data into training and testing sets. Same splits as Naive Bayes.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1234)\n",
    "\n",
    "# Initialize the classifer\n",
    "clf = LinearSVC()\n",
    "\n",
    "# Fit the model using the training data\n",
    "# generated above.\n",
    "clf_fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_predict = clf_fit.predict(X_test)\n",
    "\n",
    "# Output performance metrics\n",
    "print('Precision = %s' % precision_score(y_test, y_predict))\n",
    "print('Recall = %s' % recall_score(y_test, y_predict))\n",
    "print('F1 score = %s' % f1_score(y_test, y_predict))\n",
    "print(\"Accuracy score = %s\" % accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the `SVC()` version of the model in the exact same way. However, `SVC` is more flexible and can handle nonlinear hyperplanes. Let's use the popular [radial basis function kernel](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) to fit a nonlinear classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifer using the rbf kernel\n",
    "clf = SVC(kernel = 'rbf')\n",
    "\n",
    "# Fit the model using the training data\n",
    "# generated above.\n",
    "clf_fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_predict = clf_fit.predict(X_test)\n",
    "\n",
    "# Output performance metrics\n",
    "print('Precision = %s' % precision_score(y_test, y_predict))\n",
    "print('Recall = %s' % recall_score(y_test, y_predict))\n",
    "print('F1 score = %s' % f1_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch -- not as good of a model! We can look more at where the model is getting confused by viewing the confusion matrix (note: truth is on the rows and prediction is on the columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So complexity does not always help for **out-of-sample** classification. We can also use `SVC()` with a linear kernel to get very similar results as `LinearSVC()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifer\n",
    "clf = SVC(kernel = 'linear')\n",
    "\n",
    "# Fit the model using the training data\n",
    "# generated above.\n",
    "clf_fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_predict = clf_fit.predict(X_test)\n",
    "\n",
    "# Output performance metrics\n",
    "print('Precision = %s' % precision_score(y_test, y_predict))\n",
    "print('Recall = %s' % recall_score(y_test, y_predict))\n",
    "print('F1 score = %s' % f1_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression classifer\n",
    "\n",
    "My desert island classifer is logistic regression. We would use the same sytax to estmate a logistic regression in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifer\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Fit the model using the training data\n",
    "# generated above.\n",
    "clf_fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_predict = clf_fit.predict(X_test)\n",
    "\n",
    "# Output performance metrics\n",
    "print('Precision = %s' % precision_score(y_test, y_predict))\n",
    "print('Recall = %s' % recall_score(y_test, y_predict))\n",
    "print('F1 score = %s' % f1_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "\n",
    "As we discussed in class, one of the most important parameters in an SVC is the regularization parameter `C`. Let's see how one would \"tune\" this parameter in `sklearn` using the **grid search** method. Here, we define a (coarse) grid of parameter values, estimate a classifier, cross-validate to get out-of-sample performance, and save the results. `sklearn` has a `GridSearchCV` class that makes this very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set of the parameter grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "param_grid = [\n",
    "  {'C': [.5, 1, 1.5], 'loss': ['hinge']},\n",
    "  {'C': [.5, 1, 1.5], 'loss': ['squared_hinge']},\n",
    " ]\n",
    "\"\"\"\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [.95, .98, 1, 1.02, 1.03]}\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lsvc = LinearSVC()\n",
    "\n",
    "# Intialize grid search\n",
    "grid = GridSearchCV(lsvc, param_grid=param_grid, cv=5, scoring = 'f1')\n",
    "\n",
    "# Run grid search\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the grid search algorithm, we can print out the optimal results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best model based on CV =\")\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "print(\"\\nThe score of the best model = \")\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on parameter tuning in `sklearn` (including a number of great worked examples), see http://scikit-learn.org/stable/modules/grid_search.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating a final model\n",
    "\n",
    "We will often spend quite a bit of time in the model selection phase, trying different models, tuning parameters, and comparing out-of-sample performance. However, once we've decided on a model, then we need to use the model to classify unannotated data.\n",
    "\n",
    "This is quite easy in `sklearn` and not much changes in terms of implementation. First, we re-estimate our model based on **all available data** and then project out to the remaining sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-estimate the best fitting model on all\n",
    "# of the data\n",
    "clf = LinearSVC(C = 1.03)\n",
    "\n",
    "# Fit the model\n",
    "clf_fit = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to prepare our unannotated data using the same vectorization process as used for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unlabeled = vectorizer.transform(texts_unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we predict the new classes as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_predict = clf_fit.predict(X_unlabeled)\n",
    "print(y_predict[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we \"pretended\" that our the remaining 500 reviews were unlabeled, we can actually check the performance of our classifer for the remaining 500:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the class values for the final 500\n",
    "positive_unlabeled = positive[1500:]\n",
    "\n",
    "# Save as a numpy array\n",
    "y_unlabeled = np.array(positive_unlabeled)\n",
    "\n",
    "# Compare via the F1 score\n",
    "print('F1 score = %s' % f1_score(y_unlabeled, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n",
    "Up until now, we have assumed that we have two classes: positive and negative reviews. What if we have more classes? It turns out that extending our binary classifiers to handle multiclass data is quite easy in `sklearn`. We will, however, need a new dataset. Let's use some data from my own research on classifying skeptical claims related to climate change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read \"CARDS\" JSON file\n",
    "with open('cards.json', 'r') as jfile:\n",
    "    cards = json.load(jfile)\n",
    "\n",
    "print('Loaded %s annotated paragraphs.' % len(cards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cards` data object is a list of dictionaries with the following fields:\n",
    "\n",
    "* **id**: A unique ID for each paragraph.\n",
    "* **claim**: The skeptical \"claim\" associated with the paragraph\n",
    "* **processed_text**: The pre-processed text content of each paragraph (lowercase and simple stopword removal).\n",
    "\n",
    "The **claim** variable consists of the following \"classes\":\n",
    "\n",
    "* 0 = No claim\n",
    "* 1 = It's not happening\n",
    "* 2 = It's not us\n",
    "* 3 = It won't be bad and could be beneficial\n",
    "* 4 = Climate solutions won't work\n",
    "* 5 = Climate science is unreliable\n",
    "\n",
    "Here's an example of the what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cards[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get our data ready for `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse text and classes\n",
    "texts = [row['processed_text'] for row in cards]\n",
    "claims = [row['claim'] for row in cards]\n",
    "\n",
    "# Vectorize. Let's use TF-IDF weighting:\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# And get the label (or class) data in a format sklearn likes:\n",
    "y = np.array(claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into testing and training sets for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-rest classification\n",
    "\n",
    "There are two primary ways to do multi-class (and multi-label) classification: **one-vs-rest** and **one-vs-one** classification. Let's start with the former. In a nutshell, one-vs-rest classification fits a **binary classifier** to each class (for us, class = 0, class = 1 ... class = 5), treating the class of interest as a 1 and everything else as a zero. The prediction is carried out by simply choosing the class with the highest predicted probability.\n",
    "\n",
    "We can carryout this process for **any classifer** in `sklearn` by using the `OneVsRestClassifier()` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Initialize 1 v rest classifer using a\n",
    "# linear SVC\n",
    "clf=OneVsRestClassifier(LinearSVC())\n",
    "\n",
    "# Fit the training data\n",
    "clf_fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the new classes\n",
    "y_predict_1vrest = clf_fit.predict(X_test)\n",
    "\n",
    "print('Here are the first 10 predictions:')\n",
    "print(y_predict_1vrest[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-one classification\n",
    "\n",
    "The second option is to train a **one-vs-one** classifer. Here, we train binary classifer for each \"pair\" of classes (e.g., class 0 vs. class 5) and choose the \"predicted\" class by majority voting (i.e., the class that is predicted the most in all of the pairwise comparisons).\n",
    "\n",
    "In `sklearn`, we use the `OneVsOneClassifer` class to do this sort of classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "# Initialize 1 v rest classifer using a\n",
    "# linear SVC\n",
    "clf=OneVsOneClassifier(LinearSVC())\n",
    "\n",
    "# Fit the training data\n",
    "clf_fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the new classes\n",
    "y_predict_1v1 = clf_fit.predict(X_test)\n",
    "\n",
    "print('Here are the first 10 predictions:')\n",
    "print(y_predict_1v1[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance for multiclass problems\n",
    "\n",
    "We check performance in a very similar way as well. Let's start by looking at the **F1 score** for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OneVRest: F1 score for each class = \")\n",
    "print(f1_score(y_test, y_predict_1vrest, average = None))\n",
    "\n",
    "print(\"\\nOneVOne: F1 score for each class = \")\n",
    "print(f1_score(y_test, y_predict_1v1, average = None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one major difference between binary and multi-class performance evaluation centers on the `average` parameter -- i.e., to get overall model performance we need to average in some way. How should we take this average? There are two options: **micro-averaging** and **macro-averaging**. In `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OneVRest: macro-averaged F1 score for each class = \")\n",
    "print(f1_score(y_test, y_predict_1vrest, average = 'macro'))\n",
    "\n",
    "print(\"\\nOneVRest: micro-averaged F1 score for each class = \")\n",
    "print(f1_score(y_test, y_predict_1vrest, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that is quite a bit of difference between the two averages! What's going on here? Answer: **class imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance\n",
    "\n",
    "Why is the performance so bad for the above model? Why are the macro and micro averages so different? One reason is class imbalance. Let's look at the frequency of the various classes in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq(obj):\n",
    "    unique, counts = np.unique(obj, return_counts=True)\n",
    "    print(np.asarray((unique, counts)).T)\n",
    "    return unique, counts\n",
    "\n",
    "print(get_freq(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"no claim\" (0) class is far more frequent then the other classes. This is an example of imbalanced classes, which is a major headache for using supervised learning algorithms in practice. What can we do?\n",
    "\n",
    "One potential solution that could help is **weighting**. This solution is easy to implement in sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force a more balanced class weighting\n",
    "clf=OneVsRestClassifier(LinearSVC(class_weight='balanced'))\n",
    "\n",
    "# Fit the training data\n",
    "clf_fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the new classes\n",
    "y_predict_1vrest = clf_fit.predict(X_test)\n",
    "\n",
    "print(\"Per-class F1 score = \")\n",
    "print(f1_score(y_test, y_predict_1vrest, average = None))\n",
    "\n",
    "print(\"\\nOneVRest: macro-averaged F1 score for each class = \")\n",
    "print(f1_score(y_test, y_predict_1vrest, average = 'macro'))\n",
    "\n",
    "print(\"\\nOneVRest: micro-averaged F1 score for each class = \")\n",
    "print(f1_score(y_test, y_predict_1vrest, average = 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other ways (aside from weighting) that you can approach the class imbalance problem. While we do not have time to look at all the available solutions, many options are available in the imbalanced-learn library in Python (see http://contrib.scikit-learn.org/imbalanced-learn/stable/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0db7a017d97a45ffea759374b98e80b051fa13973580c065a6b8f6e28c7ab80d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
