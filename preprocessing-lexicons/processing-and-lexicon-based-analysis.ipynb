{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Today's challenge: a sentiment analysis of Trump tweets\n",
    "\n",
    "The goal for today is to introduce Python programming via a real world text analysis task: conducting a sentiment analysis of Twitter data. Our learning objectives include:\n",
    "\n",
    "1. Introducing Python via a real-world text analysis task.\n",
    "2. Illustrating how to load data into (and write data out of) Python\n",
    "3. Learning to implement \"standard\" text pre-processing in Python\n",
    "4. Learning how to conduct a lexicon-based sentiment analysis\n",
    "\n",
    "To achieve these objectives, we will focus on a dataset which contains all of Donald Trump's tweets for the year 2017. Our challenge is to build a simple lexicon (or dictionary) to track negative sentiment in Trump's Twitter feed. Let's get started!\n",
    "\n",
    "#### Note on programming in Python\n",
    "\n",
    "This notebook offers an introduction to progamming in the Python language. It's impossible to cover it all in a single notebook (or a single class!); however, this notebook highlights core aspects of Python that are important for this class. I highly recommend the (free and online!) book <a href=https://python.swaroopch.com/><i>A Byte of Python</i></a> if you would like to further study the ideas outlined in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the required libraries\n",
    "\n",
    "The first step in constructing our sentiment analysis script is load any required libraries using the `import` statement: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tcoan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/tcoan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os # Needed to change your working directory\n",
    "os.chdir('/Users/tcoan/git_repos/notebooks') # CHANGE THIS DIRECTORY!\n",
    "import json # Needed to load the trump_tweets_2017.json file\n",
    "\n",
    "# We will use a couple of regular expressions in this tutorial.\n",
    "# I have another notebook that provides more details on working with\n",
    "# regular expressions.\n",
    "import re\n",
    "\n",
    "# Import NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')     # Download NLTK \"data\" once (see\n",
    "nltk.download('stopwords') # https://www.nltk.org/data.html)\n",
    "\n",
    "# Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stemming and lemmatization\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Demonstrate pre-built sentiment lexicons\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Import the pandas library using the namespace \"pd\" to \n",
    "# save on typing. We will use pandas for reading/writing data,\n",
    "# as well as for the occasional descriptive statistic\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if there is a library that you need to use that is not pre-installed with Anaconda, you can install it within Jupyter using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read text data into Python for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to read in tweets for our sentiment analysis. The trump_tweets_2017.json is a <a href=\"https://www.w3schools.com/js/js_json_intro.asp\">JSON</a> formatted file and so we need to use the `json` library to open it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The \"opens\" a connection to the trump_tweets data on disk and \"loads\"\n",
    "# into an object called \"tweets\"\n",
    "with open('data/trump_tweets_2017.json', 'r', encoding=\"utf-8\") as jfile:\n",
    "    tweets = json.load(jfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we've \"loaded\" our JSON file, but what is actually stored in <b>variable</b> `tweets`? We could print it, but that doesn't help all that much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that JSON files are read as a list of dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and writing data with `pandas`\n",
    "\n",
    "We use the `json` library to read our Trump data, but what if your data is saved in something other than JSON? Python has standalone libraries for reading files stored in different formats (e.g., the <a href=https://docs.python.org/3/library/csv.html>csv</a> module). However, the `pandas` library offers a convienent approach to reading and writing files in pretty much any format out there.\n",
    "\n",
    "Note that `pandas` offers so, so much more than just reading and writing files. It's provides an R-like environment for data wrangling and analysis, and is quickly becoming the \"go to\" library for data analysis in Python. For an introduction to `pandas`, please check out:\n",
    "\n",
    "<https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/>\n",
    "\n",
    "However, we won't spend too much time on `pandas` in this section of the course, but instead use it for reading/writing data, as well as a bit of descriptive statistics.\n",
    "\n",
    "Reading data in `pandas` is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Trump tweets CSV into a pandas \"dataframe\"\n",
    "tweets_df = pd.read_csv('data/trump_tweets_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tweets_df` object is a `pandas` dataframe, which operates similarly to an R data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['retweet_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert our `trump_df` to list of dictionaries as above, we need to run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets_df.to_dict('records')\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">WARNING</span>: The code written below to clean and process our text data assumes that our data is a list of dictionaries. If you try to use this code directly on a `pandas` dataframe it will not work. If using `pandas` to read data for this class, make sure to add `to_dict('records')` when loading, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('data/trump_tweets_2017.csv').to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: prepare text for analysis (aka pre-processing)\n",
    "\n",
    "With our list of tweets in hand, we are now need to get out text ready for analysis. While the process of \"pre-processing\" text will vary a bit based on the specific analysis employed, we will cover the so-called \"standard\" pre-processing procedures:\n",
    "\n",
    "1. Tokenization\n",
    "2. Convert to lower (or upper) case\n",
    "3. Punctuation removal\n",
    "4. Stopword removal\n",
    "\n",
    "Other common pre-processing procedures **not** covered in this notebook include:\n",
    "\n",
    "5. Expanding contractions\n",
    "6. Lemmatizing or stemming\n",
    "7. Removing numbers\n",
    "\n",
    "I will cover these additional pre-processing procedures in a seperate notebook (extended-preprocessing.ipynb) for anyone that is interested.\n",
    "\n",
    "Great, we are ready to get going. Before processing our text, however, we need to get a sense of how Python understands `strings`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings and string methods\n",
    "\n",
    "Python has all of the standard data types (number, strings, bool), but `strings` (or text!) is obviously quite important for text analysis class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets[0]['text']\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings are \"iterable\", meaning that they have an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also have a set of \"methods\" (or functions) that will be useful for us throughout the course. Here are some of the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`lower()` and `upper()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'This text NEEDS to be lowercase.'.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`strip()` white space from the ends of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'   We need to strip the white space off of this sentence.         '.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`split()` a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could go on and on here. The point is do yourself a favor and review the following methods:\n",
    "\n",
    "<a href=\"https://www.w3schools.com/python/python_ref_string.asp\">https://www.w3schools.com/python/python_ref_string.asp</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is just the process of splitting up our strings into smaller parts (usually sentences or words). We've already seen one way to do this using the `split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = tweet.split(' ')\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `NLTK` for tokenization (fast, but less accurate)\n",
    "\n",
    "There are also times when you want to first tokenize a string into sentences and then into words. Here, we can use the `sent_tokenize` function that we imported above to do the job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(tweet)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simple twitter sentiment analysis, we will ignore the sentence structure and treat our text as a <b>bag of words</b>. We'll also use the `word_tokenize` function from `nltk` to get a cleaner set of tokens than what was produced using `split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = word_tokenize(tweet)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, so now we know how to tokenize a single tweet. However, we have 2,275 tweets to process. <b>Soluton</b>: the trusty old `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_dict = f'This is {len(tweets)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `spacy` for tokenization (slow, but accurate)\n",
    "\n",
    "In terms of accuracy, `spacy` is by far the best approach to tokenization in Python. To use `spacy`, we need to `import` the library and load one of the pre-trained `spacey` models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load one of the spacy models\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would then tokenize a text into sentences using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The outrageously corrupt fake news media is lying to you about this election. The Democrats attempted the most brazen and outrageous election theft in history. We need to stop the steal!\"\n",
    "parsed = nlp(text) # parsed is a spacy object with tons of goodness included!\n",
    "sents = list(parsed.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The outrageously corrupt fake news media is lying to you about this election.\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the first sentence\n",
    "print(sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize our sentences into words as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The, outrageously, corrupt, fake, news, media, is, lying, to, you, about, this, election, .]\n"
     ]
    }
   ],
   "source": [
    "toks = list(sents[0])\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PUNCT'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks[-1].pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note when working with `spacy` is that even though the above `toks` list looks like a string when printed, it's actually a `spacy` **object** which has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outrageously\n",
      "ADV\n",
      "advmod\n",
      "corrupt\n"
     ]
    }
   ],
   "source": [
    "# What's the second word in the first sentence?\n",
    "print(toks[1])\n",
    "# What's the part of speech (POS) of the second word in the first sentence?\n",
    "print(toks[1].pos_)\n",
    "# What's the dependency?\n",
    "print(toks[1].dep_)\n",
    "# What adjective is the word modifying?\n",
    "print(toks[1].head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the text strings, then we need to extract them from the `spacy` object using a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'outrageously', 'corrupt', 'fake', 'news', 'media', 'is', 'lying', 'to', 'you', 'about', 'this', 'election', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for tok in toks:\n",
    "    tokens.append(tok.text)\n",
    "\n",
    "# Let's take a look:\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can confirm that these are now strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can actually write this code a bit more efficiently by using what is called \"<b>list comprehension</b>\" in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'outrageously', 'corrupt', 'fake', 'news', 'media', 'is', 'lying', 'to', 'you', 'about', 'this', 'election', '.']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tokens_ = [tok.text for tok in toks]\n",
    "print(tokens_)\n",
    "\n",
    "# We can confirm that the list comprehension results are the same as the loop:\n",
    "print(tokens_ == tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing to note is that if you don't care about tokenizing into sentences and just want the words, you can directly work with the `parsed` object created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'outrageously', 'corrupt', 'fake', 'news', 'media', 'is', 'lying', 'to', 'you', 'about', 'this', 'election', '.', 'The', 'Democrats', 'attempted', 'the', 'most', 'brazen', 'and', 'outrageous', 'election', 'theft', 'in', 'history', '.', 'We', 'need', 'to', 'stop', 'the', 'steal', '!']\n"
     ]
    }
   ],
   "source": [
    "toks = [tok.text for tok in parsed]\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and normalizing an entire corpus ( `for` loop)\n",
    "\n",
    "Once we understand how to tokenize a single text, then tokenizing an entire corpus of tweets is easy -- we just **loop** over each tweet and tokenize. It's also a good idea to do some additional **normalization** (i.e., removing punctuation, converting to lowercase) looping over our tweets. For instance, using our `tweets` data loaded above and assuming that we wanted to use `spacy` for tokenizations, we could use the following \"clunky\" loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for tweet in tweets:\n",
    "    # Parse the tweet text using spacy\n",
    "    parsed = nlp(tweet['text'])\n",
    "    tweet_tokens = [] # Preallocate a list to hold the tokens for each tweet\n",
    "    for tok in parsed:\n",
    "        # If the token is not punctuation, save it\n",
    "        if tok.pos_ != 'PUNCT':\n",
    "            # Save the lowercase version of the string\n",
    "            tweet_tokens.append(tok.text.lower())\n",
    "    # Save the tweet tokens and move to the next\n",
    "    tokens.append(tweet_tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jobs', 'are', 'kicking', 'in', 'and', 'companies', 'are', 'coming', 'back', 'to', 'the', 'u.s.', 'unnecessary', 'regulations', 'and', 'high', 'taxes', 'are', 'being', 'dramatically', 'cut', 'and', 'it', 'will', 'only', 'get', 'better', 'much', 'more', 'to', 'come']\n"
     ]
    }
   ],
   "source": [
    "# Let's view the first tweet to see if this worked\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the \"clunky\" loop is fine, but that's not how I would have done this! I would have used something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that tokenize and normalize\n",
    "def tokenize(text):\n",
    "    '''\n",
    "       Takes a text string, tokenizes the text using spacy,\n",
    "       removes punctuation, and converts to lowercase.\n",
    "    '''\n",
    "    return [tok.text.lower() for tok in nlp(text) if tok.pos_ != 'PUNCT']\n",
    "\n",
    "# Use list comprehension to loop over the tweets and process\n",
    "tokens = [tokenize(tweet['text']) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: green;\">Activity 1</span> : tokenizing tweets using `NLTK`\n",
    "\n",
    "We've seen how to tokenize and normalize using `spacy`. However, to test your knowledge, let's see if you can carry out these tasks using `NLTK` instead. Before you get going, let's take a quick look at how the `word_tokenize()` function deals with punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our use of the `word_tokenize()` function makes removing punctuation very, very easy. Check out how `word_tokenize()` handles punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'this', 'class', '!', '!', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize('I love this class!!!!!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, all we need to do is remove strings with a length == 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the the activity!\n",
    "\n",
    "1. **Tokenize**: Loop over each \"tweet\" in our `tweets` data and apply the `word_tokenize` function\n",
    "2. **Remove punctuation**: Only keep tokens with a length greater than 1.\n",
    "3. **Lowercase**: Return each the lower case of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_nltk = []\n",
    "for tweet in tweets:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords\n",
    "\n",
    "In many analyses, it makes sense to remove so-called **stopwords**. These are words that show up frequently in a text, but add very little meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Pull in NLTK's English language stopword list.\n",
    "stops = stopwords.words('english')\n",
    "print(stops[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing to notice is our stopword list assumes lowercase. You always need to check this! But how do we remove these words from our list? Let's to it the long way first and then we will use the equivalent list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stops = [] # preallocate a list to hold tweet-level data\n",
    "# First loop is over tokenized tweets\n",
    "for toks in tokens:\n",
    "    toks_no_stops = [] # preallocate a list to hold token-level data\n",
    "    # Next loop over the individual words in each tokenized tweet\n",
    "    for tok in toks:\n",
    "        # Check if the word (tok) is in the stopword list.\n",
    "        # Need to add .lower()!!!\n",
    "        if tok.lower() not in set(stops):\n",
    "            toks_no_stops.append(tok)\n",
    "    # Store result and move to next tweet\n",
    "    tokens_no_stops.append(toks_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with stopwords removed:  ['Jobs', 'kicking', 'companies', 'coming', 'back', 'U.S.', 'Unnecessary', 'regulations', 'high', 'taxes', 'dramatically', 'Cut', ',', 'get', 'better', '.', 'MUCH', 'COME', '!'] \n",
      "\n",
      "Original tokens:  ['Jobs', 'are', 'kicking', 'in', 'and', 'companies', 'are', 'coming', 'back', 'to', 'the', 'U.S.', 'Unnecessary', 'regulations', 'and', 'high', 'taxes', 'are', 'being', 'dramatically', 'Cut', ',', 'and', 'it', 'will', 'only', 'get', 'better', '.', 'MUCH', 'MORE', 'TO', 'COME', '!']\n"
     ]
    }
   ],
   "source": [
    "print('Tokens with stopwords removed: ', tokens_no_stops[0], '\\n')\n",
    "print('Original tokens: ', tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a more compact version using list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stops = [] # preallocate a list to hold tweet-level data\n",
    "for toks in tokens:\n",
    "    tokens_no_stops.append([tok for tok in toks if tok not in set(stops)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is still some junk in this tweet (e.g., 'amp', 'https', etc.). It's not uncommon to have \"corpus-specific\" (or \"extended\") stopwords. We can add additional words to the stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'amp', 'https', 'co']\n"
     ]
    }
   ],
   "source": [
    "stops_to_add = ['amp', 'https', 'co']\n",
    "\n",
    "# Concatenate the two lists\n",
    "extended_stops = stops + stops_to_add\n",
    "\n",
    "# The last stopword is now 'co'\n",
    "print(extended_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to use this extended stop word list and we are good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stops = [] # preallocate a list to hold tweet-level data\n",
    "for toks in tokens:\n",
    "    tokens_no_stops.append([tok for tok in toks if tok not in set(extended_stops)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build and use your lexicon (or dictionary)\n",
    "\n",
    "What words should go into our lexicon of \"negative\" words? While there are a number of words with clear, negative conotations, lexicon-based approaches often need to be tailored to a specific task (or at least validated for the specific task at hand). We will start by illustrating how to build and employ our own lexicon -- as this could be useful across a range of tasks, not just sentiment analysis -- and then I will quickly demonstate how to use existing sentiment lexicons in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting simple: looking up a single word\n",
    "\n",
    "Let's start simple and examine the presence of absence of a single word in our corpus: fake. Here's one way of completing this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = []\n",
    "for toks in tokens_no_stops:\n",
    "    if 'fake' in toks:\n",
    "        fake.append(1)\n",
    "    else:\n",
    "        fake.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'fake' in ' '.join(tokens_no_stops[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building our negative sentiment lexicon\n",
    "\n",
    "A lexicon (or dictonary) is just a list of words. That's it! So we can start by created list of \"negative\" words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = ['fake', 'hoax', 'idiot', 'moron', 'phony', 'fight', 'dishonest', 'unfair']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Okay, okay, this list isn't exhaustive, but it is good enough to show you how to implement your own lexicon-based approach. Using our `negative` words lexicon takes a little code, but we have all of the tools to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tweet does not have text. Setting negative sentiment to 0.\n"
     ]
    }
   ],
   "source": [
    "# Start by looping over each list of tokens\n",
    "negative_sentiment_score = []\n",
    "for toks in tokens_no_stops:\n",
    "    # Define a counter to hold the number of negative words\n",
    "    negative_words = 0\n",
    "    # It is possible for a tweet not to have text. If this is the case,\n",
    "    # we need to skip it.\n",
    "    if len(toks) == 0:\n",
    "        print(\"This tweet does not have text. Setting negative sentiment to 0.\")\n",
    "        negative_sentiment_score.append(0) \n",
    "    else:\n",
    "        # Then loop over each token in each list\n",
    "        for tok in toks:\n",
    "            if tok in set(negative):\n",
    "                negative_words += 1\n",
    "        # Add normalized sentiment score\n",
    "        negative_sentiment_score.append(negative_words/len(toks)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.17857142857142858,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06896551724137931,\n",
       " 0.0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sentiment_score[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-built sentiment lexicons\n",
    "\n",
    "There are a number of good sentiment lexicons already available and `NLTK` provides access to a number of these (see the <a href=\"https://www.nltk.org/api/nltk.sentiment.html\">nltk.sentiment api documentation</a> for a full list). Once lexicon that has been shown to work well across a range of tasks is <a href=\"http://scholar.google.co.uk/scholar_url?url=https://ojs.aaai.org/index.php/ICWSM/article/download/14550/14399&hl=en&sa=X&ei=rUoRYKXUDIfCmgGXoLToAw&scisig=AAGBfm22NmYm4wMvPAvkQZuGz-24V1Wu1A&nossl=1&oi=scholarr\">VADER</a>. Here's how you can use VADER in `NLTK`. First, start by instantiating the `SentimentIntensityAnalyzer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/tcoan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.sentiment.vader.SentimentIntensityAnalyzer object at 0x7f9e62d14280>\n"
     ]
    }
   ],
   "source": [
    "print(vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.556, 'neu': 0.444, 'pos': 0.0, 'compound': -0.3612}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.polarity_scores(\"This class sucks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no pre-processing necessary -- you can simply pass your text in \"raw\" form. Applying to our Trump example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    # Here I'm adding a new field to the \"tweets\" dictionary directly\n",
    "    # While this changes the original dictionry (and so requires caution),\n",
    "    # it ensures that we have all of the relevant meta-data about the tweet\n",
    "    # easily accessble.\n",
    "    tweet['negative_sentiment'] = vader.polarity_scores(tweet['text'])['neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'Twitter for iPhone', 'id_str': 9.47e+17, 'text': 'I use Social Media not because I like to, but because it is the only way to fight a VERY dishonest and unfair “press,” now often referred to as Fake News Media. Phony and non-existent “sources” are being used more often than ever. Many stories &amp; reports a pure fiction!', 'is_retweet': False, 'retweet_count': 50342, 'favorite_count': 195754, 'negative_sentiment': 0.344}\n"
     ]
    }
   ],
   "source": [
    "print(tweets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_write = pd.DataFrame(tweets).to_csv('tweets_sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus material: defining a pre-processing function using `NLTK`\n",
    "\n",
    "It's good practice to roll up our various bits of pre-processing in a single **function**. Functions allow to reuse pieces (or blocks) of code. We do so by declaring a function using the `def` statement. We have already used several of Python's built-in functions earlier in this tutoral. For instance, we \"called\" the `len` function to get the number of characters in a string. Python, however, makes it super easy to define your own functions.\n",
    "\n",
    "For example, we can combine the main pre-processing steps above into a single function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet, stops):\n",
    "    '''\n",
    "    Helper function to pre-process tweet text. It takes an individual tweet,\n",
    "    tokenizes it, converts to lowers, and removes punctuation.\n",
    "    \n",
    "    Args:\n",
    "        tweet (str): The (unprocessed) tweet text\n",
    "        stops (list): Stopword list to use\n",
    "    \n",
    "    Returns:\n",
    "        list of lists: Returns processed tokens\n",
    "    '''\n",
    "    # Convert the entire tweet to lowercase, tokenize, and remove stop words\n",
    "    tokens = [token for token in word_tokenize(tweet.lower()) \n",
    "          if token not in set(stops)]\n",
    "    # Remove punctuation\n",
    "    tokens_no_punct = [token for token in tokens if len(token) > 1]\n",
    "    # Remove numbers and return the processed list of tokens\n",
    "    return tokens_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(process_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then process our corpus using a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = [process_tweet(tweet['text'], extended_stops) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_clean[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set up a function to use our lexicon in the same exact way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lexicon (tokens, lexicon):\n",
    "    '''\n",
    "    Takes a tokenized set of texts and counts the number of tokens\n",
    "    in each text included in a lexicon.\n",
    "    \n",
    "    Args:\n",
    "        tokens (list of list): Tokenized text\n",
    "        lexicon (list of str): List of tokens to lookup\n",
    "    '''\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        lexicon_words = 0\n",
    "        if len(token) == 0:\n",
    "            result.append(None) \n",
    "        else:\n",
    "            # Then loop over each token in each list\n",
    "            for tok in token:\n",
    "                if tok in set(lexicon):\n",
    "                    lexicon_words += 1\n",
    "            result.append(lexicon_words/len(toks)) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calculate_lexicon(tweets_clean, negative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0db7a017d97a45ffea759374b98e80b051fa13973580c065a6b8f6e28c7ab80d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
